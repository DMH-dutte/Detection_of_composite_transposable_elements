{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of composite transposable elements \n",
    "## Notebook 3: Testing TF Models\n",
    "## Description:\n",
    "Transposable elements are sequences in genomes that can change their position in the genome. Thus, they are also called “jumping genes”. They are able to affect the composition and size of genetic replicons. Our research interest in this project are composite transposable elements, which are flanked by two inverted repeats and transposable elements. Composite transposable elements are moving as one unit within a genome and are copying and inserting genes enclosed by itself. The following traits of composite transposable elements are making their detection challenging: \n",
    "\n",
    "1. Sometimes terminal information such as repeats or transposable elements are missing, which would theoretically determine the boundaries of a composite transposable element.\n",
    "2. Composite transposable elements are diverse in their genetic composition and size. \n",
    "\n",
    "Composite transposable elements are usually associated with essential and indispensable genes, which are having a high gene frequency across genomes, but also with genes of lower essentiality, which leads to significant drop in the gene frequency landscape. We hypothesize that the genetic frequency landscape of a replicon will follow a particular pattern, which can be used as a marker for putative regions of composite transposable elements. Thus, we are representing here an approach to detect regions of putative composite transposable elements using gene frequencies, protein family clusters and positions of composite transposable elements as input for a supervised LSTM-based neural network model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Repo \n",
    "https://github.com/DMH-dutte/Detection_of_composite_transposable_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participants:\n",
    "Dustin Martin Hanke: dhanke@ifam.uni-kiel.de\n",
    "\n",
    "Wang Yiging: ywang@ifam.uni-kiel.de "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course and Semester\n",
    "Machine Learning with TensorFlow - Wintersemester 2021/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import datetime\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input data is: (29360, 25, 2)\n",
      "The shape of the labels is: (29360, 1)\n"
     ]
    }
   ],
   "source": [
    "#Arrays have been stored as 1D-arrays and are reshaped directly into the correct format\n",
    "two_d_4 = np.loadtxt('../arrays/two_d_4.csv', delimiter=',').reshape(29360, 25, 2)\n",
    "labels4 = np.loadtxt('../arrays/labels4.csv', delimiter=',').reshape(29360, 1)\n",
    "print(\"The shape of the input data is: {}\".format(two_d_4.shape))\n",
    "print(\"The shape of the labels is: {}\".format(labels4.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes of the splitted input data and labels:\n",
      "(23488, 25, 2) (5872, 25, 2) (23488, 1) (5872, 1)\n",
      "Distribution:\n",
      "Positive samples: 50.0%\n",
      "Negative samples: 50.0%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(two_d_4, labels4, test_size=0.2, random_state=42, stratify=labels4)\n",
    "print(\"The shapes of the splitted input data and labels:\")\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(\"Distribution:\")\n",
    "print(\"Positive samples: {}%\".format((np.sum(y_train)/len(X_train))*100))\n",
    "print(\"Negative samples: {}%\".format((np.sum(y_test)/len(X_test))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_plt(history, x):\n",
    "    plt.figure(x)\n",
    "    plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Testing accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"graphs/accuracy_{}.png\".format(datetime.datetime.now()))\n",
    "    return\n",
    "\n",
    "def loss_plt(history, x):\n",
    "    plt.figure(x)\n",
    "    plt.plot(history.history['loss'], label='Training loss')\n",
    "    plt.plot(history.history['val_loss'], label='Testing loss')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"graphs/loss_{}.png\".format(datetime.datetime.now()))\n",
    "    return\n",
    "\n",
    "def rate_loss_log(lrs, history, x):\n",
    "    plt.figure(x, figsize=(18,8))\n",
    "    plt.semilogx(lrs, history.history['loss'], lw=3, color='#000')\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"graphs/learning_rate_{}.png\".format(datetime.datetime.now()))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Baseline1():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True, input_shape=(25, 2)))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "def Baseline2():\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(25, 2)))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "def Baseline3():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(25, 2)))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "def Baseline4():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(25, 2)))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(units=16, activation=\"sigmoid\"))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "baseline1 = Baseline1()\n",
    "baseline2 = Baseline2()\n",
    "baseline3 = Baseline3()\n",
    "baseline4 = Baseline4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation to find the optimal learning rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_start = 1e-6 \n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_train(model, batch_size, num_epochs, lr_start):\n",
    "    num_train_steps = (len(y_train) // batch_size) * num_epochs\n",
    "    opt = Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    \n",
    "    # tensorboard callback\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # save weights callback\n",
    "    #checkpoint_path = \"training_2/biLSTM_composite_TE.ckpt\"\n",
    "    #checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    #cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)\n",
    "    \n",
    "    #Learning rate callback\n",
    "    callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_start * 10 ** (epoch/30))\n",
    "    \n",
    "    #Fit model\n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1, shuffle=True, callbacks=[callback]) \n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution to find the optimal learning rate for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = lr_start * (10 ** (np.arange(epochs)/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_train(baseline1, 32, epochs, lr_start)\n",
    "accuracy_plt(history, 1)\n",
    "loss_plt(history, 2)\n",
    "rate_loss_log(learning_rates, history, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model1/learning_rate/loss.png?raw=true) ![Accuracy](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model1/learning_rate/accuracy.png?raw=true) ![Learning Rate](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model1/learning_rate/learning_rate.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_train(baseline2, 32, epochs, lr_start)\n",
    "accuracy_plt(history, 4)\n",
    "loss_plt(history, 5)\n",
    "rate_loss_log(learning_rates, history, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model2/learning_rate/loss.png?raw=true) ![Accuracy](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model2/learning_rate/accuracy.png?raw=true) ![Learning Rate](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model2/learning_rate/learning_rate.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_train(baseline3, 32, epochs, lr_start)\n",
    "accuracy_plt(history, 7)\n",
    "loss_plt(history, 8)\n",
    "rate_loss_log(learning_rates, history, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model3/learning_rate/loss.png?raw=true) ![Accuracy](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model3/learning_rate/accuracy.png?raw=true) ![Learning Rate](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model3/learning_rate/learning_rate.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_train(baseline4, 32, epochs, lr_start)\n",
    "accuracy_plt(history, 10)\n",
    "loss_plt(history, 11)\n",
    "rate_loss_log(learning_rates, history, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model4/learning_rate/loss.png?raw=true) ![Accuracy](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model4/learning_rate/accuracy.png?raw=true) ![Learning Rate](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model4/learning_rate/learning_rate.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_train_fixed(model, batch_size, num_epochs, lr, checkpoint_path):\n",
    "\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    \n",
    "    # tensorboard callback\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # save weights callback\n",
    "    checkpoint_path = checkpoint_path\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)\n",
    "    callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr)\n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1, shuffle=True, callbacks=[cp_callback, callback]) #reduce_lr for Plateau approach\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose learning rates from loss vs. log(learing rate) graphs above:\n",
    "1. Model1: 15e-4\n",
    "2.  Model2: 90e-5\n",
    "3.  Model3: 20e-5\n",
    "\n",
    "You can find the output of the training process here: \"/outputs/model_training.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_train_fixed(baseline1, 32, 300, 15e-4, \"models/model1.cp\")\n",
    "accuracy_plt(history, 13)\n",
    "loss_plt(history, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model1/training/loss_after.png?raw=true) ![Accuracy](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model1/training/accuracy_after.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_train_fixed(baseline2, 32, 300, 90e-5, \"models/model2.cp\")\n",
    "accuracy_plt(history, 15)\n",
    "loss_plt(history, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model2/training/loss_after.png?raw=true) ![Accuracy](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model2/training/accuracy_after.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_train_fixed(baseline3, 32, 300, 20e-5, \"models/model3.cp\")\n",
    "accuracy_plt(history, 17)\n",
    "loss_plt(history, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](graphs/model1/learning_rate/loss.png) ![Accuracy](https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model3/training/accuracy_after.png?raw=true)\n",
    "\n",
    "<img src=\"https://github.com/DMH-dutte/Detection_of_composite_transposable_elements/blob/main/code/graphs/model3/training/loss_after.png\"/>\n",
    "<img src=\"/graphs/model3/training/loss_after.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "1. Model 1: We didn't hit the optimal learning rate for model 1. It might be hard to hit the optimal learning rate, because the slope of the drop in loss is very steep. We guess that there is a very specific optimal learning rate that is necessary to choose and we suggest to calculate it.\n",
    "2. Model 2: The model starts to increase its performance signifcantly after 115 epochs and above. Nonetheless, the accuracy and loss are dropping a lot and the results doesn't seem to be stable.\n",
    "3. Model 3: Two LSTMs are yielding satisfing results, since the accuracy and loss are improving very fast after about 20 epochs. Though, somtimes drops in accuracy and loss are observable, but overall it seems that the model is still learning afeter 200-300 epochs without divering training and test results. Thus, we estimate that this model is performing better than the others due to its stability and fast increase of the accuracy and decrease of the loss. \n",
    "4. So far, we suggest a model that is based on two LSTMs layers. We observed that an accuracy of 89 % can be reached and this kind of model still indicates potential for better learning. However, a fine-tuning of the parameters is still possible, but it's important to avoid overfitting.\n",
    "5. We have been demonstrating that our approach to detect composite transposable elements based on gene frequency and protein families might be possible due to our binary classification approach. The binary classification indicates that the model can learn the pattern of gene frequency and protein families and that the signal are strong enough to detect composite transposable elements. We recommend to build a model for specific boundary predictions of composite transposable elements and to clean the input dataset towards confident and strong signals of composite transposable elements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
